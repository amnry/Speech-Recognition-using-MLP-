{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdheme-J02J0",
        "outputId": "48e98260-7b02-4eb2-b537-de643c1d5621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# If workig in drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gcS4x6o31BhA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v1 as tf\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk_b16rd1OEr",
        "outputId": "977e8107-a769-4ac5-b561-aad1e3e12b76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded\n"
          ]
        }
      ],
      "source": [
        "def load_raw():\n",
        "    return (\n",
        "        np.load(r'/Users/amanarya/Colab Notebooks/IDC410/Assignment_4_Speech_recognition_MLP_NN/DATA/dev.npy', encoding='bytes', allow_pickle = True),        \n",
        "        np.load(r'/Users/amanarya/Colab Notebooks/IDC410/Assignment_4_Speech_recognition_MLP_NN/DATA/dev_labels.npy', encoding='bytes', allow_pickle = True)\n",
        "    )\n",
        "data, labels = load_raw()   #loads the feature numpy array onto data and the label file onto labels\n",
        "print(\"Loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqErExsE1SHd",
        "outputId": "4864039a-8971-492d-bb87-4a3205e282e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(669294, 40) (669294,)\n"
          ]
        }
      ],
      "source": [
        "x_data, y_data = np.array([]), np.array([])                 #the data numpy array was of shape 1108(no. of files/matrices) x 338(no. of frames/vectors in each matrices) x 40\n",
        "for i in range(len(data)):                                  #It was flattened into a 2d array of 669294(total no. of frames/vectors) x 40\n",
        "    if i==0:                                                #The labels numpy array of 1108 x 338 was also flattened into a 1d array of 669294 elements\n",
        "        x_data = data[0]                                    #The input array is now x_data and output array is y_data\n",
        "        y_data = labels[0]\n",
        "    else:\n",
        "        x_data = np.concatenate((x_data, data[i]))\n",
        "        y_data = np.concatenate((y_data, labels[i]))\n",
        "print(x_data.shape, y_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbbUGoEq1cFg",
        "outputId": "66f9ee97-cd2f-4165-a69e-ed87a7e16902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(602364, 40) (602364,)\n",
            "(66930, 40) (66930,)\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.1, random_state = 42)     #The x_data and y_data are divided into test and train sets in the ratio 9:1\n",
        "print(x_train.shape, y_train.shape)                                                                         \n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvYuV-ZB1hol",
        "outputId": "b9f4db00-3242-4ab0-fa33-f81a6592097f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(602364, 138) (66930, 138)\n"
          ]
        }
      ],
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train)            #The y_data is further modulated as a set of probabilities assigned to 138 categories\n",
        "y_test = tf.keras.utils.to_categorical(y_test)              #For example, y_train value of 2 is converted into an array [0, 0, 1, 0 ... 0] of size 138 that now serves as output array\n",
        "print(y_train.shape, y_test.shape)                                          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3UTEuyZC1pXc"
      },
      "outputs": [],
      "source": [
        "graph = tf.Graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIzr4sD92hhn",
        "outputId": "68e1d2f7-9290-46b5-b7dd-912f5695a946"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 138)\n",
            "Layers defined\n",
            "Loss and optimiser defined\n",
            "Accuracy computation nodes included\n"
          ]
        }
      ],
      "source": [
        "with graph.as_default():\n",
        "  x = tf.placeholder(tf.float32, shape = [None, 40])                                                  #The input layer has 40 nodes and the output has 138 nodes\n",
        "  y = tf.placeholder(tf.float32, shape = [None, 138])\n",
        "\n",
        "  #layer 1, dim = 70\n",
        "  layer_1 = tf.Variable(tf.truncated_normal([40, 60]), trainable = True, name = 'layer_1' )           #For modelling, 3 hidden layers (60, 80, 100 nodes) were used\n",
        "  layer_1_out = tf.matmul(x, layer_1)\n",
        "  layer_1_out = tf.sigmoid(layer_1_out)\n",
        "\n",
        "  #layer 2, dim = 100\n",
        "  layer_2 = tf.Variable(tf.truncated_normal([60, 80]), trainable = True, name = 'layer_2' )\n",
        "  layer_2_out = tf.matmul(layer_1_out, layer_2)\n",
        "  layer_2_out = tf.sigmoid(layer_2_out)\n",
        "\n",
        "  layer_3 = tf.Variable(tf.truncated_normal([80, 100]), trainable = True, name = 'layer_3' )\n",
        "  layer_3_out = tf.matmul(layer_2_out, layer_3)\n",
        "  layer_3_out = tf.sigmoid(layer_3_out)\n",
        "\n",
        "  #layer 3, dim = 138\n",
        "  layer_4 = tf.Variable(tf.truncated_normal([100, 138]), trainable = True, name = 'layer_4' )\n",
        "  layer_4_out = tf.matmul(layer_3_out, layer_4)\n",
        "  print(layer_4_out.get_shape())\n",
        "  print(\"Layers defined\")\n",
        "  \n",
        "  loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = y, logits = layer_4_out, name = \"loss\")    #Softmax function was used to categorise the output i.e. assign probabilities to the 138 nodes as per the input\n",
        "  avg_loss = tf.reduce_mean(loss)                                                                       #The loss function used is mean squared error between predicted set of probabilities and y_train\n",
        "  optimizer = tf.train.AdamOptimizer(0.01).minimize(avg_loss)                                           #Adam Optimizer is used with learning rate 0.01, which was found to be better suited for this data through hit and trial\n",
        "  print(\"Loss and optimiser defined\")\n",
        "\n",
        "  #accuracy\n",
        "  probs = tf.nn.softmax(layer_4_out)                                                                    #The set of predicted probabilities from the last layer\n",
        "  preds = tf.argmax(probs, axis = -1)                                                                   #The node with the maximum predicted probability is taken as the predicted label/output for that particular input/frame/vector\n",
        "  hyps = tf.argmax(y, axis = -1)                                                                        #The actual label for the given input/frame/vector\n",
        "  corrects = tf.cast(tf.equal(hyps, preds), tf.float32)                                                 #The total no. of matches between predicted labels and actual labels\n",
        "  acc = tf.reduce_mean(corrects)                                                                        #The percentage of matches scaled to 1 is defined as accuracy\n",
        "  print(\"Accuracy computation nodes included\")\n",
        "\n",
        "  saver = tf.train.Saver(max_to_keep = 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKBpte986y7g",
        "outputId": "6f6fd231-45a1-444e-9291-70f724af0fd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-19 13:30:46.058849: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 / 30 LOSS 4.8717231679670885 Accuracy 0.10102034617436699\n",
            "Epoch 2 / 30 LOSS 3.6642358161435267 Accuracy 0.15543614652487311\n",
            "Epoch 3 / 30 LOSS 3.4562861730556675 Accuracy 0.17949894483726803\n",
            "Epoch 4 / 30 LOSS 3.334621521505979 Accuracy 0.19409857498537195\n",
            "Epoch 5 / 30 LOSS 3.266304983951078 Accuracy 0.20306036097578484\n",
            "Epoch 6 / 30 LOSS 3.224069269576875 Accuracy 0.20888277062095037\n",
            "Epoch 7 / 30 LOSS 3.1930894025481575 Accuracy 0.21223425614361716\n",
            "Epoch 8 / 30 LOSS 3.166796658298757 Accuracy 0.21611301541918576\n",
            "Epoch 9 / 30 LOSS 3.144581849032109 Accuracy 0.2197281496949715\n",
            "Epoch 10 / 30 LOSS 3.1265610798750774 Accuracy 0.2220324090506771\n",
            "Epoch 11 / 30 LOSS 3.111486050161985 Accuracy 0.22382411124682663\n",
            "Epoch 12 / 30 LOSS 3.0961527777190256 Accuracy 0.22631472039340747\n",
            "Epoch 13 / 30 LOSS 3.083475049179379 Accuracy 0.22861809158089139\n",
            "Epoch 14 / 30 LOSS 3.0715384766607 Accuracy 0.23023918197296633\n",
            "Epoch 15 / 30 LOSS 3.0610045702150552 Accuracy 0.23192183909439804\n",
            "Epoch 16 / 30 LOSS 3.050188156637815 Accuracy 0.23395635881046256\n",
            "Epoch 17 / 30 LOSS 3.0406267713792254 Accuracy 0.23536585950025238\n",
            "Epoch 18 / 30 LOSS 3.0303242513448887 Accuracy 0.23735810299911123\n",
            "Epoch 19 / 30 LOSS 3.021640435303792 Accuracy 0.23883525156738736\n",
            "Epoch 20 / 30 LOSS 3.014299390339615 Accuracy 0.24026428576153103\n",
            "Epoch 21 / 30 LOSS 3.0064034627215697 Accuracy 0.24170589299485234\n",
            "Epoch 22 / 30 LOSS 2.9981236457824707 Accuracy 0.24360332379836847\n",
            "Epoch 23 / 30 LOSS 2.990552522168301 Accuracy 0.2450467064829156\n",
            "Epoch 24 / 30 LOSS 2.984053540937971 Accuracy 0.24638829018810007\n",
            "Epoch 25 / 30 LOSS 2.9786304813800473 Accuracy 0.247637339600242\n",
            "Epoch 26 / 30 LOSS 2.9730212570417045 Accuracy 0.24840212163358633\n",
            "Epoch 27 / 30 LOSS 2.966487018188628 Accuracy 0.2496756791773409\n",
            "Epoch 28 / 30 LOSS 2.962595488765452 Accuracy 0.24985186136004947\n",
            "Epoch 29 / 30 LOSS 2.9570396158954884 Accuracy 0.25118925739632975\n",
            "Epoch 30 / 30 LOSS 2.952494937594574 Accuracy 0.2519787982253745\n"
          ]
        }
      ],
      "source": [
        "total = len(x_train)                                                                #Size of training dataset is 6,02,364. The batch size is taken 1/100 th of it ~ 6000.\n",
        "EPOCHS = 30                                                                         #30 iterations to be done by the optimiser for reducing loss\n",
        "batch_size = 6000\n",
        "\n",
        "with tf.Session(graph = graph) as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for e in range(EPOCHS):\n",
        "    e_loss = 0                                                                  #loss for the current iteration\n",
        "    e_acc = 0                                                                   #accuracy for the current iteration\n",
        "    start = 0                                                                   #To keep track of number of vectors of the training data set that have been analysed\n",
        "    b_count = 0                                                                 #To keep track of No. of batches that were formed\n",
        "    while start < total:\n",
        "      end = min(start + batch_size, total)\n",
        "      bx = x_train[start:end]\n",
        "      by = y_train[start:end]\n",
        "      data = {x:bx, y:by}\n",
        "      b_loss,_,b_acc = sess.run([avg_loss, optimizer, acc], feed_dict = data)\n",
        "      e_loss += b_loss\n",
        "      e_acc += b_acc\n",
        "      start = end\n",
        "      b_count += 1\n",
        "    e_loss /= b_count                                                             #loss of current iteration(epoch) averaged over no. of batches\n",
        "    e_acc /= b_count\n",
        "    saver.save(sess, 'weights/mlp')                                               #session saved for later use\n",
        "    print(\"Epoch\", e+1, \"/\", EPOCHS, \"LOSS\", e_loss, \"Accuracy\", e_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUWZQW6a9yyK",
        "outputId": "50610e51-b669-42b3-c196-c3d308155e29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from weights/mlp\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.24766174\n"
          ]
        }
      ],
      "source": [
        "random_test_index = np.array(range(0, len(x_test)))                                                                 #a set of indices created to choose from test data\n",
        "\n",
        "test_batch_x = x_test[random_test_index]\n",
        "test_batch_y = y_test[random_test_index]\n",
        "\n",
        "with tf.Session(graph = graph) as sess:\n",
        "  saver.restore(sess, 'weights/mlp')                                                                            #restoring the saved session\n",
        "  test_batch = {x:test_batch_x, y:test_batch_y}                                                                 #to pass the set of x_test and y_test data to the session\n",
        "  prediction, hypothesis, probabilities, accuracy = sess.run([preds, hyps, probs, acc], feed_dict = test_batch) #to extract predicted labels, actual labels and accuracy feeding test data to the model\n",
        "  \n",
        "  data = {'ID':list(random_test_index), 'True Label':list(hypothesis), 'Predicted Label':list(prediction)}      #dataframe created with columns ID, True Label and Predicted label\n",
        "  df = pd.DataFrame.from_dict(data)\n",
        "  df.to_csv(r'/Users/amanarya/Colab Notebooks/IDC410/Assignment_4_Speech_recognition_MLP_NN/OUTPUT.csv', index = False)       #dataframe written into a csv file\n",
        "  print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
